{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [[1,1,'maybe'],\n",
    "               [1,1,'yes'],\n",
    "               [1,0,'no'],\n",
    "               [0,1,'no'],\n",
    "               [0,1,'no']]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    return dataSet,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算熵,dataSet为list，里面的每条数据也都是list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcShannonEnt(dataSet): \n",
    "    numEntries = len(dataSet) #dataSet的样本量\n",
    "    labelCounts = {}   #dict\n",
    "    for featVec in dataSet:   #featVec也是list\n",
    "        currentLabel = featVec[-1]  #每条数据最后一列为label  eg:[1,0,'yes']\n",
    "        if currentLabel not in labelCounts.keys():   \n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1    #分别计算所有类别的数据个数\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key] / numEntries)\n",
    "        shannonEnt -= prob * log(prob,2)  #熵\n",
    "    return shannonEnt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照给定特征划分数据集，dataSet为待划分数据集，axis为划分数据集的特征，value为需要返回的特征的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitDataSet (dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet :\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]  #从头开始取到第axis-1个元素\n",
    "            reducedFeatVec.extend(featVec[axis+1:])  #把从第axis+1个到最后一个元素插入reducedFeatVec中\n",
    "            retDataSet.append(reducedFeatVec) #将reducedFeatVec作为一个list插入retDataSet中\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择最好的数据集划分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1 #特征的个数\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]  #所有数据第i个特征的取值\n",
    "        uniqueVals = set(featList)  #第i的特征的可能取值 set中元素不可重复\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)  #按照第i个特征的某个取值value划分数据集\n",
    "            prob = len(subDataSet)/float(len(dataSet))  #特征i的取值等于value的数据所占概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet) #按照特征i划分之后的熵\n",
    "        infoGain = baseEntropy - newEntropy #信息增益\n",
    "        if (infoGain > bestInfoGain):  #计算最大的信息增益\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "采用多数表决的方法决定叶子结点中数据的分类，classList为同一叶子结点中的数据的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True) \n",
    "    #将classCount分解为元祖，按照第2个元素即classCount进行排序，逆序：从大到小\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建树，dataSet，labels为数据集中所有特征的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTree(dataSet,labels):\n",
    "    classList = [example[-1] for example in dataSet] #数据集的所有类标签\n",
    "    if classList.count(classList[0]) == len(classList):  #count(classList[0])为统计classList[0]出现的次数\n",
    "        return classList[0]  #如果dataSet中所有的类标签完全相同则直接返回该类标签\n",
    "    if len(dataSet[0]) == 1:  #dataSet第一条数据中只剩标签，即已经使用完了所有特征\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet) #bestFeat返回最佳分类特征的索引\n",
    "    bestFeatLabel = labels[bestFeat]  #bestFeat的标签\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet] #数据中该特征的所有取值\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]  #labels[bestfeat]已经被删除\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) #递归创建子树\n",
    "    return myTree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(inputTree,featLabels,testVec):\n",
    "    firstStr = inputTree.keys()[0]\n",
    "    secondDict = inputTree[firstStr]\n",
    "    featIndex = featLabels.index(firstStr)  #查找当前列表中第一个匹配 firstStr的元素\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featLabelsIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel =classify(secondDict[key],featLabels,testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeTree(inputTree,filename):\n",
    "    import pickle\n",
    "    fw = open(filename,'w')\n",
    "    pickle.dump(inputTree,fw)\n",
    "    fw.close()\n",
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    fr = open(filename)\n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook trees.ipynb to python\n",
      "[NbConvertApp] Writing 4283 bytes to trees.py\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    !jupyter nbconvert --to python trees.ipynb\n",
    "    # python即转化为.py，script即转化为.html\n",
    "    # file_name.ipynb即当前module的文件名\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataSet,labels = createDataSet()\n",
    "#chooseBestFeatureToSplit(dataSet)\n",
    "#myTree = createTree(dataSet,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storeTree(myTree,'classifierStorage.txt')\n",
    "#grabTree('classifierStorage.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
